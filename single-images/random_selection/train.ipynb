{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfb0c1-6e31-410f-b44f-f2313578e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae2137-7fca-439b-9a8e-0b7c94b09332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec556f0-b698-4de4-aa2f-9871ba4b1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': v2.Compose([\n",
    "        v2.RandomRotation(30),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "\n",
    "        # v2.Resize(224),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "\n",
    "    ]),\n",
    "    'val': v2.Compose([\n",
    "        # v2.Resize(224),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': v2.Compose([\n",
    "        # v2.Resize(224),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f0483-072e-4dff-b27f-5b38a47d5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/dental-images'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=16)\n",
    "                for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"input is a tensor\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "# get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    history = {'train': {'loss': [], 'acc': []}, 'val': {'loss': [], 'acc': []}}\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "    \n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                history[phase]['loss'].append(epoch_loss)\n",
    "                # acc is a tensor, so we need to convert it to a float\n",
    "                history[phase]['acc'].append(epoch_acc.item())\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d37276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c482f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model_ft = models.shufflenet_v2_x2_0(weights='IMAGENET1K_V1')\n",
    "# model_ft = models.shufflenet_v2_x1_5(weights='IMAGENET1K_V1')\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# model_ft = models.mnasnet1_3(weights='IMAGENET1K_V1')\n",
    "# num_ftrs = model_ft.classifier[1].in_features\n",
    "\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft, history_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history in two subplots, one for loss and one for accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_ft['train']['loss'], label='train')\n",
    "plt.plot(history_ft['val']['loss'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_ft['train']['acc'], label='train')\n",
    "plt.plot(history_ft['val']['acc'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9525a2c-a09a-4b42-856d-6bc1e78d1580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show confusion matrix of best model\n",
    "\n",
    "model_ft.eval()\n",
    "preds = []\n",
    "true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloaders['val']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        true.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(true, preds)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "seaborn.heatmap(df_cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# show recall, precision and f1-score\n",
    "print(classification_report(true, preds, target_names=class_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fef206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model and plot images wrongly classified\n",
    "model_ft.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "correct_images = []\n",
    "correct_labels = []\n",
    "correct_preds = []\n",
    "wrong_images = []\n",
    "wrong_labels = []\n",
    "wrong_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloaders['test']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "\n",
    "        correct_images.extend(inputs[labels == pred])\n",
    "        correct_labels.extend(labels[labels == pred])\n",
    "        correct_preds.extend(pred[labels == pred])\n",
    "        \n",
    "        wrong = pred != labels\n",
    "        wrong_images.extend(inputs[wrong])\n",
    "        wrong_labels.extend(labels[wrong])\n",
    "        wrong_preds.extend(pred[wrong])\n",
    "\n",
    "\n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct / total}%')\n",
    "\n",
    "print(f'Images wrongly classified: {len(wrong_images)}')\n",
    "\n",
    "for i in range(len(wrong_images)):\n",
    "    imshow(wrong_images[i].cpu(), title=f'predicted: {class_names[wrong_preds[i]]}, true: {class_names[wrong_labels[i]]}, subject: {image_datasets[\"test\"].samples[i][0]}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(f'Images correctly classified: {len(correct_images)}')\n",
    "for i in range(5):\n",
    "    imshow(correct_images[i].cpu(), title=f'predicted: {class_names[correct_preds[i]]}, true: {class_names[correct_labels[i]]}, subject: {image_datasets[\"test\"].samples[i][0]}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df9ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43862f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# print number of parameters in millions\n",
    "print(f'Number of parameters: {num_params / 1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# print number of parameters in millions\n",
    "print(f'Number of parameters: {num_params / 1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with a smaller model\n",
    "model = models.shufflenet_v2_x0_5(weights='IMAGENET1K_V1')\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# print number of parameters in millions\n",
    "print(f'Number of parameters: {num_params / 1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller model\n",
    "model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# print number of parameters in millions\n",
    "print(f'Number of parameters: {num_params / 1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c729258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_tiny',\n",
       " 'convnext_small',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_s',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_l',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_8gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_32gf',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnext50_32x4d',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'wide_resnet50_2',\n",
       " 'wide_resnet101_2',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'vit_h_14',\n",
       " 'swin_t',\n",
       " 'swin_s',\n",
       " 'swin_b',\n",
       " 'swin_v2_t',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_b',\n",
       " 'maxvit_t',\n",
       " 'get_model',\n",
       " 'get_model_builder',\n",
       " 'get_model_weights',\n",
       " 'get_weight',\n",
       " 'list_models']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all pretrained models in torchvision\n",
    "import torchvision.models\n",
    "pretrained_models = [name for name in torchvision.models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(torchvision.models.__dict__[name])]\n",
    "pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f30795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "/home/antonio/miniconda3/envs/ml/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for get_model: Unknown model get_model\n",
      "Error for get_model_builder: Unknown model get_model_builder\n",
      "Error for get_model_weights: Unknown model get_model_weights\n",
      "Error for get_weight: Unknown model get_weight\n",
      "Error for list_models: Unknown model list_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/antonio/.cache/torch/hub/pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# get number of parameters for each model pretrained on ImageNet\n",
    "model_params = {}\n",
    "for model_name in pretrained_models:\n",
    "    try:\n",
    "        weight_enum = torch.hub.load(\"pytorch/vision\", \"get_model_weights\", model_name)\n",
    "        for weight in weight_enum:\n",
    "            model = getattr(torchvision.models, model_name)(weights=weight)\n",
    "            num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "            # remove model name from weight\n",
    "            weight = str(weight).split('.')[-1]\n",
    "            model_params[f'{model_name}_{weight}'] = {'num_params': num_params}\n",
    "            \n",
    "    \n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error for {model_name}: {e}')\n",
    "\n",
    "\n",
    "# sort models by number of parameters\n",
    "sorted_models = sorted(model_params.items(), key=lambda x: x[1]['num_params'], reverse=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac959efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regnet_y_128gf_IMAGENET1K_SWAG_E2E_V1: 644.81M\n",
      "regnet_y_128gf_IMAGENET1K_SWAG_LINEAR_V1: 644.81M\n",
      "vit_h_14_IMAGENET1K_SWAG_E2E_V1: 633.47M\n",
      "vit_h_14_IMAGENET1K_SWAG_LINEAR_V1: 632.05M\n",
      "vit_l_32_IMAGENET1K_V1: 306.54M\n",
      "vit_l_16_IMAGENET1K_SWAG_E2E_V1: 305.17M\n",
      "vit_l_16_IMAGENET1K_V1: 304.33M\n",
      "vit_l_16_IMAGENET1K_SWAG_LINEAR_V1: 304.33M\n",
      "convnext_large_IMAGENET1K_V1: 197.77M\n",
      "regnet_y_32gf_IMAGENET1K_V1: 145.05M\n",
      "regnet_y_32gf_IMAGENET1K_V2: 145.05M\n",
      "regnet_y_32gf_IMAGENET1K_SWAG_E2E_V1: 145.05M\n",
      "regnet_y_32gf_IMAGENET1K_SWAG_LINEAR_V1: 145.05M\n",
      "vgg19_bn_IMAGENET1K_V1: 143.68M\n",
      "vgg19_IMAGENET1K_V1: 143.67M\n",
      "vgg16_bn_IMAGENET1K_V1: 138.37M\n",
      "vgg16_IMAGENET1K_V1: 138.36M\n",
      "vgg16_IMAGENET1K_FEATURES: 138.36M\n",
      "vgg13_bn_IMAGENET1K_V1: 133.05M\n",
      "vgg13_IMAGENET1K_V1: 133.05M\n",
      "vgg11_bn_IMAGENET1K_V1: 132.87M\n",
      "vgg11_IMAGENET1K_V1: 132.86M\n",
      "wide_resnet101_2_IMAGENET1K_V1: 126.89M\n",
      "wide_resnet101_2_IMAGENET1K_V2: 126.89M\n",
      "efficientnet_v2_l_IMAGENET1K_V1: 118.52M\n",
      "regnet_x_32gf_IMAGENET1K_V1: 107.81M\n",
      "regnet_x_32gf_IMAGENET1K_V2: 107.81M\n",
      "resnext101_32x8d_IMAGENET1K_V1: 88.79M\n",
      "resnext101_32x8d_IMAGENET1K_V2: 88.79M\n",
      "convnext_base_IMAGENET1K_V1: 88.59M\n",
      "vit_b_32_IMAGENET1K_V1: 88.22M\n",
      "swin_v2_b_IMAGENET1K_V1: 87.93M\n",
      "swin_b_IMAGENET1K_V1: 87.77M\n",
      "vit_b_16_IMAGENET1K_SWAG_E2E_V1: 86.86M\n",
      "vit_b_16_IMAGENET1K_V1: 86.57M\n",
      "vit_b_16_IMAGENET1K_SWAG_LINEAR_V1: 86.57M\n",
      "regnet_y_16gf_IMAGENET1K_V1: 83.59M\n",
      "regnet_y_16gf_IMAGENET1K_V2: 83.59M\n",
      "regnet_y_16gf_IMAGENET1K_SWAG_E2E_V1: 83.59M\n",
      "regnet_y_16gf_IMAGENET1K_SWAG_LINEAR_V1: 83.59M\n",
      "resnext101_64x4d_IMAGENET1K_V1: 83.46M\n",
      "wide_resnet50_2_IMAGENET1K_V1: 68.88M\n",
      "wide_resnet50_2_IMAGENET1K_V2: 68.88M\n",
      "efficientnet_b7_IMAGENET1K_V1: 66.35M\n",
      "alexnet_IMAGENET1K_V1: 61.10M\n",
      "resnet152_IMAGENET1K_V1: 60.19M\n",
      "resnet152_IMAGENET1K_V2: 60.19M\n",
      "regnet_x_16gf_IMAGENET1K_V1: 54.28M\n",
      "regnet_x_16gf_IMAGENET1K_V2: 54.28M\n",
      "efficientnet_v2_m_IMAGENET1K_V1: 54.14M\n",
      "convnext_small_IMAGENET1K_V1: 50.22M\n",
      "swin_v2_s_IMAGENET1K_V1: 49.74M\n",
      "swin_s_IMAGENET1K_V1: 49.61M\n",
      "resnet101_IMAGENET1K_V1: 44.55M\n",
      "resnet101_IMAGENET1K_V2: 44.55M\n",
      "efficientnet_b6_IMAGENET1K_V1: 43.04M\n",
      "regnet_x_8gf_IMAGENET1K_V1: 39.57M\n",
      "regnet_x_8gf_IMAGENET1K_V2: 39.57M\n",
      "regnet_y_8gf_IMAGENET1K_V1: 39.38M\n",
      "regnet_y_8gf_IMAGENET1K_V2: 39.38M\n",
      "maxvit_t_IMAGENET1K_V1: 30.92M\n",
      "efficientnet_b5_IMAGENET1K_V1: 30.39M\n",
      "densenet161_IMAGENET1K_V1: 28.68M\n",
      "convnext_tiny_IMAGENET1K_V1: 28.59M\n",
      "swin_v2_t_IMAGENET1K_V1: 28.35M\n",
      "swin_t_IMAGENET1K_V1: 28.29M\n",
      "inception_v3_IMAGENET1K_V1: 27.16M\n",
      "resnet50_IMAGENET1K_V1: 25.56M\n",
      "resnet50_IMAGENET1K_V2: 25.56M\n",
      "resnext50_32x4d_IMAGENET1K_V1: 25.03M\n",
      "resnext50_32x4d_IMAGENET1K_V2: 25.03M\n",
      "resnet34_IMAGENET1K_V1: 21.80M\n",
      "efficientnet_v2_s_IMAGENET1K_V1: 21.46M\n",
      "densenet201_IMAGENET1K_V1: 20.01M\n",
      "regnet_y_3_2gf_IMAGENET1K_V1: 19.44M\n",
      "regnet_y_3_2gf_IMAGENET1K_V2: 19.44M\n",
      "efficientnet_b4_IMAGENET1K_V1: 19.34M\n",
      "regnet_x_3_2gf_IMAGENET1K_V1: 15.30M\n",
      "regnet_x_3_2gf_IMAGENET1K_V2: 15.30M\n",
      "densenet169_IMAGENET1K_V1: 14.15M\n",
      "efficientnet_b3_IMAGENET1K_V1: 12.23M\n",
      "resnet18_IMAGENET1K_V1: 11.69M\n",
      "regnet_y_1_6gf_IMAGENET1K_V1: 11.20M\n",
      "regnet_y_1_6gf_IMAGENET1K_V2: 11.20M\n",
      "regnet_x_1_6gf_IMAGENET1K_V1: 9.19M\n",
      "regnet_x_1_6gf_IMAGENET1K_V2: 9.19M\n",
      "efficientnet_b2_IMAGENET1K_V1: 9.11M\n",
      "densenet121_IMAGENET1K_V1: 7.98M\n",
      "efficientnet_b1_IMAGENET1K_V1: 7.79M\n",
      "efficientnet_b1_IMAGENET1K_V2: 7.79M\n",
      "shufflenet_v2_x2_0_IMAGENET1K_V1: 7.39M\n",
      "regnet_x_800mf_IMAGENET1K_V1: 7.26M\n",
      "regnet_x_800mf_IMAGENET1K_V2: 7.26M\n",
      "googlenet_IMAGENET1K_V1: 6.62M\n",
      "regnet_y_800mf_IMAGENET1K_V1: 6.43M\n",
      "regnet_y_800mf_IMAGENET1K_V2: 6.43M\n",
      "mnasnet1_3_IMAGENET1K_V1: 6.28M\n",
      "regnet_x_400mf_IMAGENET1K_V1: 5.50M\n",
      "regnet_x_400mf_IMAGENET1K_V2: 5.50M\n",
      "mobilenet_v3_large_IMAGENET1K_V1: 5.48M\n",
      "mobilenet_v3_large_IMAGENET1K_V2: 5.48M\n",
      "efficientnet_b0_IMAGENET1K_V1: 5.29M\n",
      "mnasnet1_0_IMAGENET1K_V1: 4.38M\n",
      "regnet_y_400mf_IMAGENET1K_V1: 4.34M\n",
      "regnet_y_400mf_IMAGENET1K_V2: 4.34M\n",
      "mobilenet_v2_IMAGENET1K_V1: 3.50M\n",
      "mobilenet_v2_IMAGENET1K_V2: 3.50M\n",
      "shufflenet_v2_x1_5_IMAGENET1K_V1: 3.50M\n",
      "mnasnet0_75_IMAGENET1K_V1: 3.17M\n",
      "mobilenet_v3_small_IMAGENET1K_V1: 2.54M\n",
      "shufflenet_v2_x1_0_IMAGENET1K_V1: 2.28M\n",
      "mnasnet0_5_IMAGENET1K_V1: 2.22M\n",
      "shufflenet_v2_x0_5_IMAGENET1K_V1: 1.37M\n",
      "squeezenet1_0_IMAGENET1K_V1: 1.25M\n",
      "squeezenet1_1_IMAGENET1K_V1: 1.24M\n"
     ]
    }
   ],
   "source": [
    "# print sorted models\n",
    "for model, params in sorted_models:\n",
    "    print(f'{model}: {params[\"num_params\"] / 1e6:.2f}M')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c37469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sorted_models to a json file\n",
    "import json\n",
    "\n",
    "with open('sorted_models.json', 'w') as f:\n",
    "    json.dump(sorted_models, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
